{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimator 的优势\n",
    "- 用于针对分布式环境训练模型, 例如用大型数据集进行分布式训练并导出模型以用于生产,此外，您可以在 CPU、GPU 或 TPU 上运行基于 Estimator 的模型，而无需重新编码模型。\n",
    "- Estimator 本身在 tf.layers 之上构建而成，可以简化自定义过程。\n",
    "- Estimator 会为您构建图。\n",
    "- Estimator 提供安全的分布式训练循环，可以控制如何以及何时：\n",
    "  - 构建图\n",
    "  - 初始化变量\n",
    "  - 开始排队\n",
    "  - 处理异常\n",
    "  - 创建检查点文件并从故障中恢复\n",
    "  - 保存 TensorBoard 的摘要\n",
    "\n",
    "使用 Estimator 编写应用时，必须将数据输入管道从模型中分离出来。这种分离简化了不同数据集的实验流程。\n",
    "\n",
    "## 预创建的 Estimator 程序的结构\n",
    "1.编写一个或多个数据集导入函数。 例如，您可以创建一个函数来导入训练集，并创建另一个函数来导入测试集。每个数据集导入函数都必须返回两个对象：\n",
    "- 一个字典，其中键是特征名称，值是包含相应特征数据的张量（或 SparseTensor）\n",
    "- 一个包含一个或多个标签的张量\n",
    "```python\n",
    "def input_fn(dataset):\n",
    "   ...  # manipulate dataset, extracting the feature dict and the label\n",
    "   return feature_dict, label\n",
    "```\n",
    "\n",
    "2.定义特征列。 每个 tf.feature_column 都标识了特征名称、特征类型和任何输入预处理操作。例如，以下代码段创建了三个存储整数或浮点数据的特征列。前两个特征列仅标识了特征的名称和类型。第三个特征列还指定了一个 lambda，该程序将调用此 lambda 来调节原始数据：\n",
    "```python\n",
    "population = tf.feature_column.numeric_column('population')\n",
    "crime_rate = tf.feature_column.numeric_column('crime_rate')\n",
    "median_education = tf.feature_column.numeric_column('median_education',\n",
    "                    normalizer_fn=lambda x: x - global_education_mean)\n",
    "```\n",
    "\n",
    "3.实例化相关的预创建的 Estimator\n",
    "```python\n",
    "estimator = tf.estimator.LinearClassifier(\n",
    "    feature_columns=[population, crime_rate, median_education],\n",
    "    )\n",
    "```\n",
    "\n",
    "4.调用训练、评估或推理方法\n",
    "```python\n",
    "estimator.train(input_fn=my_training_set, steps=2000)\n",
    "```\n",
    "\n",
    "## 自定义 Estimator\n",
    "每个 Estimator（无论是预创建还是自定义）的核心都是其模型函数，这是一种为训练、评估和预测构建图的方法。\n",
    "我们推荐以下工作流程：\n",
    "1. 假设存在合适的预创建的 Estimator，使用它构建第一个模型并使用其结果确定基准。\n",
    "2. 使用此预创建的 Estimator 构建和测试整体管道，包括数据的完整性和可靠性。\n",
    "3. 如果存在其他合适的预创建的 Estimator，则运行实验来确定哪个预创建的 Estimator 效果最好。\n",
    "4. 可以通过构建自定义 Estimator 进一步改进模型。\n",
    "\n",
    "### 从 Keras 模型创建 Estimator\n",
    "调用 tf.keras.estimator.model_to_estimator将现有的 Keras 模型转换为 Estimator\n",
    "```\n",
    "est_inception_v3 = tf.keras.estimator.model_to_estimator(keras_model=keras_inception_v3)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.获取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "    y_name = 'Species'\n",
    "\n",
    "    train_path = './data/iris_training.csv'\n",
    "    test_path = './data/iris_test.csv'\n",
    "    \n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "    return (train_x, train_y), (test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.9</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          6.4         2.8          5.6         2.2\n",
       "1          5.0         2.3          3.3         1.0\n",
       "2          4.9         2.5          4.5         1.7\n",
       "3          4.9         3.1          1.5         0.1\n",
       "4          5.7         3.8          1.7         0.3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(train_x, train_y), (test_x, test_y)=load_data()\n",
    "train_x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.创建输入函数\n",
    "您必须创建输入函数来提供用于训练、评估和预测的数据。\n",
    "\n",
    "输入函数是 返回tf.data.Dataset对象，此对象接收参数是 含有两个元素的元组：\n",
    "\n",
    "- features：包含原始输入特征的 {'feature_name':array} 字典（或 DataFrame）。\n",
    "- labels：包含每个样本的标签的数组。\n",
    "- batch_size：表示所需批次大小的整数  \n",
    "\n",
    "```\n",
    "features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "            'SepalWidth':  np.array([2.8, 2.3]),\n",
    "            'PetalLength': np.array([5.6, 3.3]),\n",
    "            'PetalWidth':  np.array([2.2, 1.0])}\n",
    "labels = np.array([2, 1])\n",
    "```\n",
    "\n",
    "### tf.data.Dataset.from_tensor_slices\n",
    " tf.data.Dataset.from_tensor_slices 函数创建一个代表数组切片的 tf.data.Dataset。系统会在第一个维度内对该数组进行切片。例如，一个包含 MNIST 训练数据的数组的形状为 (60000, 28, 28)。将该数组传递给 from_tensor_slices 会返回一个包含 60000 个切片的 Dataset 对象，其中每个切片都是一个 28x28 的图像\n",
    "```python\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices(mnist_x)\n",
    "```\n",
    "这段代码将输出以下行，显示数据集中item的形状和类型。请注意，Dataset 不知道自己包含多少item\n",
    "```\n",
    "<TensorSliceDataset shapes: (28,28), types: tf.uint8>\n",
    "```\n",
    "\n",
    "Dataset 可以透明地处理字典或元组（或 namedtuple）的任何嵌套组合。例如，在将鸢尾花 features 转换为标准 Python 字典后，您可以将数组字典转换为字典 Dataset\n",
    "```python\n",
    "dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "```\n",
    "```\n",
    "<TensorSliceDataset\n",
    "    shapes: (\n",
    "        {\n",
    "          SepalLength: (), PetalWidth: (),\n",
    "          PetalLength: (), SepalWidth: ()},\n",
    "        ()),\n",
    "\n",
    "    types: (\n",
    "        {\n",
    "          SepalLength: tf.float64, PetalWidth: tf.float64,\n",
    "          PetalLength: tf.float64, SepalWidth: tf.float64},\n",
    "        tf.int64)>\n",
    "```\n",
    "### 操作\n",
    "Dataset 会按固定顺序迭代数据一次，并且一次仅生成一个元素。它需要进一步处理才可用于训练\n",
    "```python\n",
    "dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "```\n",
    "- shuffle 方法使用一个固定大小的缓冲区，在条目经过时随机化处理条目。buffer_size 大于 Dataset 中样本的数量，确保数据完全被随机化处理（鸢尾花数据集仅包含 150 个样本）。\n",
    "\n",
    "- repeat 方法会在结束时重启 Dataset。要限制周期数量，请设置 count 参数。\n",
    "\n",
    "- batch 方法会收集大量样本并将它们堆叠起来以创建批次。这为批次的形状增加了一个维度。新的维度将添加为**第一个维度**。以下代码对之前的 MNIST Dataset 使用 batch 方法。\n",
    "```\n",
    "print(mnist_ds.batch(100))\n",
    "```\n",
    "```\n",
    "<BatchDataset\n",
    "  shapes: (?, 28, 28),\n",
    "  types: tf.uint8>\n",
    "```\n",
    "请注意，该数据集的批次大小是未知的，因为最后一个批次具有的元素数量会减少。\n",
    "\n",
    "在 train_input_fn 中，经过批处理之后，dataset如下所示：\n",
    "```\n",
    "print(dataset)\n",
    "```\n",
    "```\n",
    "<TensorSliceDataset\n",
    "    shapes: (\n",
    "        {\n",
    "          SepalLength: (?,), PetalWidth: (?,),\n",
    "          PetalLength: (?,), SepalWidth: (?,)},\n",
    "        (?,)),\n",
    "\n",
    "    types: (\n",
    "        {\n",
    "          SepalLength: tf.float64, PetalWidth: tf.float64,\n",
    "          PetalLength: tf.float64, SepalWidth: tf.float64},\n",
    "        tf.int64)>\n",
    "```\n",
    "### 返回\n",
    "此时，Dataset 包含 (features_dict, labels) 对。这是 train 和 evaluate 方法的预期格式，因此 input_fn 会返回相应的数据集\n",
    "\n",
    "使用 predict 方法时，可以/应该忽略 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size):\n",
    "\n",
    "    # 将输入转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    features = dict(features)\n",
    "    if labels is None:\n",
    "        inputs = features  # No labels, use only features.\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # 将inputs转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # batch the examples\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.定义特征列\n",
    "特征列是一个对象，用于说明模型应该如何使用特征字典中的原始输入数据。在构建 Estimator 模型时，向其传递一个特征列的列表，其中包含您希望模型使用的每个特征。tf.feature_column 模块提供很多用于在模型中表示数据的选项。\n",
    "![](images/1.jpg)\n",
    "![](images/2.jpg)\n",
    "对于鸢尾花问题，4 个原始特征是数值，因此我们会构建一个特征列的列表，以告知 Estimator 模型将这 4 个特征都表示为 32 位浮点值。因此，创建特征列的代码如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SepalLength\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "SepalWidth\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "PetalLength\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n",
      "PetalWidth\n",
      "[NumericColumn(key='SepalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='SepalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalLength', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None), NumericColumn(key='PetalWidth', shape=(1,), default_value=None, dtype=tf.float32, normalizer_fn=None)]\n"
     ]
    }
   ],
   "source": [
    "my_feature_columns = []\n",
    "for key in train_x.keys():\n",
    "    print(key)\n",
    "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))  # 默认dtype=tf.float32\n",
    "    print(my_feature_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.实例化 Estimator\n",
    "鸢尾花问题是一个经典的分类问题。幸运的是，TensorFlow 提供了几个预创建的分类器 Estimator，其中包括：\n",
    "\n",
    "tf.estimator.DNNClassifier：适用于执行多类别分类的深度模型。  \n",
    "tf.estimator.DNNLinearCombinedClassifier：适用于宽度和深度模型。  \n",
    "tf.estimator.LinearClassifier：适用于基于线性模型的分类器。  \n",
    "\n",
    "对于鸢尾花问题，tf.estimator.DNNClassifier 似乎是最好的选择\n",
    "```\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3\n",
    "    model_dir='models/iris'  # 检查点文件目录\n",
    "    )  \n",
    "```\n",
    "### 默认检查点目录\n",
    "第一次调用 train 会将检查点和其他文件添加到 model_dir 目录中  \n",
    "如果未在 Estimator 的构造函数中指定 model_dir，则 Estimator 会将检查点文件写入由 Python 的 tempfile.mkdtemp 函数选择的临时目录中\n",
    "```python\n",
    "print(classifier.model_dir)\n",
    "```\n",
    "\n",
    "### 检查点频率  \n",
    "**默认情况**下，Estimator 按照以下时间安排将检查点保存到 model_dir 中：  \n",
    "\n",
    "- 每 10 分钟（600 秒）写入一个检查点。  \n",
    "- 在 train 方法开始（第一次迭代）和完成（最后一次迭代）时写入一个检查点。  \n",
    "- 只在目录中保留 5 个最近写入的检查点。  \n",
    "\n",
    "您可以通过执行下列步骤来更改默认时间安排：  \n",
    "\n",
    "- 创建一个 RunConfig 对象来定义所需的时间安排。  \n",
    "- 在实例化 Estimator 时，将该 RunConfig 对象传递给 Estimator 的 config 参数。  \n",
    "\n",
    "```python\n",
    "my_checkpointing_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.\n",
    "        keep_checkpoint_max = 3,       # 只保留最近3个checkpoints.\n",
    "    )\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10],\n",
    "    n_classes=3,\n",
    "    model_dir='models/iris',\n",
    "    config=my_checkpointing_config)\n",
    "```\n",
    "\n",
    "### 恢复模型\n",
    "\n",
    "第一次调用 Estimator 的 train 方法时，TensorFlow 会将一个检查点保存到 model_dir 中。随后每次调用 Estimator 的 train、evaluate 或 predict 方法时，都会发生下列情况：\n",
    "- Estimator 通过运行 model_fn() 构建模型图。\n",
    "- Estimator 根据最近写入的检查点中存储的数据来初始化新模型的权重。  \n",
    "\n",
    "换言之，一旦存在检查点，TensorFlow 就会在您每次调用 train()、evaluate() 或 predict() 时重建模型。\n",
    "\n",
    "### 避免不当恢复\n",
    "通过检查点恢复模型的状态这一操作仅在模型和检查点兼容时可行。例如，假设您训练了一个 DNNClassifier Estimator，它包含 2 个隐藏层且每层都有 10 个节点：  \n",
    "\n",
    "在训练之后（因此已在 models/iris 中创建检查点），假设您将每个隐藏层中的神经元数量从 10 更改为 20，然后尝试重新训练模型：\n",
    "\n",
    "由于检查点中的状态与 classifier2 中描述的模型不兼容，因此重新训练失败并出现错误\n",
    "\n",
    "**可以在重新训练前删除检查点文件**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.训练、评估和预测\n",
    "- 通过调用 Estimator 的 train 方法训练模型，如下所示:\n",
    "```python\n",
    "classifier.train(\n",
    "    input_fn=lambda: train_input_fn(train_x, train_y, batch_size),\n",
    "    steps=train_steps)\n",
    "```\n",
    "将 input_fn 调用封装在 lambda 中以获取参数，同时提供一个不采用任何参数的输入函数。steps 参数告知方法在训练多步后停止训练\n",
    "\n",
    "- 调用 Estimator 的 evaluate 方法评估经过训练的模型\n",
    "```python\n",
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda: eval_input_fn(test_x, test_y, batch_size))\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "```\n",
    "eval_result的结果为：\n",
    "{'accuracy': 0.96666664, 'average_loss': 0.05575007, 'loss': 1.672502, 'global_step': 2000}\n",
    "\n",
    "与对 train 方法的调用不同，我们没有传递 steps 参数来进行评估。我们的 eval_input_fn 只生成一个epoch的数据\n",
    "\n",
    "- 调用 Estimator 的 predict 方法进行预测\n",
    "\n",
    "```python\n",
    "expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "predict_x = {\n",
    "    'SepalLength': [5.1, 5.9, 6.9],\n",
    "    'SepalWidth': [3.3, 3.0, 3.1],\n",
    "    'PetalLength': [1.7, 4.2, 5.4],\n",
    "    'PetalWidth': [0.5, 1.5, 2.1],\n",
    "}\n",
    "\n",
    "predictions = classifier.predict(\n",
    "    input_fn=lambda: eval_input_fn(predict_x, batch_size=batch_size))\n",
    "for pred_dict, expec in zip(predictions, expected):\n",
    "```\n",
    "train_x,test_x,predict_x 都要转化为python字典  \n",
    "predictions为：  \n",
    "```\n",
    "<generator object Estimator.predict at 0x7f2d514c1e08>\n",
    "```\n",
    "1个pred_dict为1个样本的预测结果：\n",
    "```\n",
    "{'logits': array([ 20.65349 ,  15.137624, -16.802929], dtype=float32), 'probabilities': array([9.9599361e-01, 4.0063257e-03, 5.3844290e-17], dtype=float32), 'class_ids': array([0]), 'classes': array([b'0'], dtype=object), 'all_class_ids': array([0, 1, 2], dtype=int32), 'all_classes': array([b'0', b'1', b'2'], dtype=object)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.967\n",
      "\n",
      "\n",
      "Prediction is \"Setosa\" (99.7%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (100.0%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (99.6%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=100\n",
    "train_steps=1000\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def load_data():\n",
    "    CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "    y_name = 'Species'\n",
    "\n",
    "    train_path = './data/iris_training.csv'\n",
    "    test_path = './data/iris_test.csv'\n",
    "    \n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "\n",
    "    # 将输入转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    features = dict(features)\n",
    "    if labels is None:\n",
    "        inputs = features  # No labels, use only features.\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # 将inputs转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # batch the examples\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 获取数据\n",
    "    (train_x, train_y), (test_x, test_y) = load_data()\n",
    "\n",
    "    # 定义特征列\n",
    "    # key是train_x的列名：SepalLength，SepalWidth，PetalLength，PetalWidth\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    my_checkpointing_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.\n",
    "        keep_checkpoint_max = 3,       # 只保留最近3个checkpoints.\n",
    "    )\n",
    "\n",
    "    # 建立2个隐藏层DNN的模型，每层10个隐藏节点\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=my_feature_columns,  #特征列\n",
    "        hidden_units=[10, 10],  #每层10个节点\n",
    "        n_classes=3,  #分为3类\n",
    "        model_dir='models/iris',  # 保存模型\n",
    "        config=my_checkpointing_config\n",
    "    )  \n",
    "\n",
    "    # 训练模型\n",
    "    classifier.train(\n",
    "        input_fn=lambda: train_input_fn(train_x, train_y, batch_size),\n",
    "        steps=train_steps)\n",
    "\n",
    "    # 评估模型\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda: eval_input_fn(test_x, test_y, batch_size))\n",
    "    \n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # 进行预测\n",
    "    # predict_x里的1列为1个样本，有3列共3个样本，expected是3个样本的真实label\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth':  [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth':  [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda: eval_input_fn(predict_x, labels=None, batch_size=batch_size))\n",
    "\n",
    "    template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(SPECIES[class_id], 100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建自定义 Estimator\n",
    "预创建的 Estimator\n",
    "![](images/3.png)\n",
    "如果需要设计独特的模型或指标，也可以编写自定义 Estimator  \n",
    "\n",
    "以下使用自定义 Estimator 解决鸢尾花问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定义模型函数\n",
    "自定义模型函数参考下面标准:\n",
    "```python\n",
    "def my_model_fn(\n",
    "   features, # This is batch_features from input_fn\n",
    "   labels,   # This is batch_labels from input_fn\n",
    "   mode,     # An instance of tf.estimator.ModeKeys\n",
    "   params):  # Additional configuration\n",
    "```\n",
    "\n",
    "前两个参数是从输入函数中返回的特征和标签批次；也就是说，features 和 labels 是模型将使用的数据的句柄。  \n",
    "mode 参数表示调用程序是请求训练、预测还是评估   \n",
    "调用程序可以将 params 传递给 Estimator 的构造函数。传递给构造函数的所有 params 转而又传递给 model_fn。\n",
    "### 定义模型\n",
    "必须定义下列三个部分：\n",
    "- 一个输入层\n",
    "- 一个或多个隐藏层\n",
    "- 一个输出层\n",
    "\n",
    "**输入层**  \n",
    "在 model_fn 的第一行调用 tf.feature_column.input_layer，以将特征字典和 feature_columns 转换为模型的输入，如下所示：\n",
    "```python\n",
    "net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "```\n",
    "**隐藏层**  \n",
    "使用tf.layers  \n",
    "在第一次迭代中，net 表示输入层。在每次循环迭代时，tf.layers.dense 都使用变量 net 创建一个新层，该层将前一层的输出作为其输入。\n",
    "```python\n",
    "for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)  #units 参数会定义指定层中输出神经元的数量。\n",
    "```\n",
    "创建两个隐藏层后，我们的网络如下所示:\n",
    "![](images/5.png)\n",
    "**输出层**  \n",
    "输出层不使用激活函数\n",
    "```python\n",
    "logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "```\n",
    "![](images/6.png)\n",
    "### 实现训练、评估和预测\n",
    "Estimator方法 | Estimator模式\n",
    "-|-\n",
    "train() | ModeKeys.TRAIN\n",
    "evaluate() | ModeKeys.EVAL\n",
    "predict() | ModeKeys.PREDICT\n",
    "\n",
    "**预测**   \n",
    "如果调用 Estimator 的 predict 方法，则 model_fn 会收到 mode = ModeKeys.PREDICT。在这种情况下，模型函数必须返回一个包含预测的 tf.estimator.EstimatorSpec\n",
    "该模型必须经过训练才能进行预测\n",
    "```python\n",
    "predicted_classes = tf.argmax(logits, 1)\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # 定义predictions输出哪些内容\n",
    "    predictions = {\n",
    "        'class_ids': predicted_classes[:, tf.newaxis],\n",
    "        'probabilities': tf.nn.softmax(logits),\n",
    "        'logits': logits,\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "```\n",
    "针对预测返回的 EstimatorSpec 通常包含 predictions 参数,用于将该字典返回到调用程序。Estimator 的 predict 方法会生成这些字典\n",
    "\n",
    "**计算损失**  \n",
    "通过调用 tf.losses.sparse_softmax_cross_entropy 计算交叉熵损失  \n",
    "此函数会针对整个批次返回平均值\n",
    "```python\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "```\n",
    "**评估**  \n",
    "如果调用 Estimator 的 evaluate 方法，则 model_fn 会收到 mode = ModeKeys.EVAL。在这种情况下，模型函数必须返回一个包含模型损失和一个或多个指标（可选）的 tf.estimator.EstimatorSpec  \n",
    "tf.metrics 来计算常用指标,tf.metrics.accuracy 函数要求标签和预测具有相同的形状\n",
    "```python\n",
    "accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                               predictions=predicted_classes,\n",
    "                               name='acc_op')\n",
    "```\n",
    "\n",
    "针对评估返回的 EstimatorSpec 通常包含:\n",
    "- loss：这是模型的损失\n",
    "- eval_metric_ops：这是可选的指标字典\n",
    "\n",
    "```python\n",
    "metrics = {'accuracy': accuracy}\n",
    "tf.summary.scalar('accuracy', accuracy[1])  #tf.summary.scalar 会在 TRAIN 和 EVAL 模式下向 TensorBoard 提供准确率\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.EVAL:\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode, loss=loss, eval_metric_ops=metrics)\n",
    "```\n",
    "**训练**  \n",
    "如果调用 Estimator 的 train 方法，则会调用 model_fn 并收到 mode = ModeKeys.TRAIN。在这种情况下，模型函数必须返回一个包含损失和训练操作的 EstimatorSpec  \n",
    "需要构建训练操作需要优化器和train_op  \n",
    "针对训练返回的 EstimatorSpec 必须包含:\n",
    "- loss：包含损失函数的值。\n",
    "- train_op：执行训练步\n",
    "\n",
    "## 实例化自定义Estimator\n",
    "通过 Estimator 基类实例化自定义 Estimator\n",
    "```python\n",
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model,\n",
    "    params={\n",
    "        'feature_columns': my_feature_columns,\n",
    "        'hidden_units': [10, 10],\n",
    "        'n_classes': 3,\n",
    "    })\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 完整代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set accuracy: 0.967\n",
      "\n",
      "\n",
      "Prediction is \"Setosa\" (99.9%), expected \"Setosa\"\n",
      "\n",
      "Prediction is \"Versicolor\" (99.9%), expected \"Versicolor\"\n",
      "\n",
      "Prediction is \"Virginica\" (95.8%), expected \"Virginica\"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=100\n",
    "train_steps=1000\n",
    "SPECIES = ['Setosa', 'Versicolor', 'Virginica']\n",
    "\n",
    "def load_data():\n",
    "    CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']\n",
    "    y_name = 'Species'\n",
    "\n",
    "    train_path = './data/iris_training.csv'\n",
    "    test_path = './data/iris_test.csv'\n",
    "    \n",
    "    train = pd.read_csv(train_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    train_x, train_y = train, train.pop(y_name)\n",
    "\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_x, test_y = test, test.pop(y_name)\n",
    "    return (train_x, train_y), (test_x, test_y)\n",
    "\n",
    "\n",
    "def train_input_fn(features, labels, batch_size):\n",
    "    # 将输入转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    # Shuffle, repeat, and batch the examples.\n",
    "    dataset = dataset.shuffle(1000).repeat().batch(batch_size)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def eval_input_fn(features, labels, batch_size):\n",
    "    features = dict(features)\n",
    "    if labels is None:\n",
    "        inputs = features  # No labels, use only features.\n",
    "    else:\n",
    "        inputs = (features, labels)\n",
    "\n",
    "    # 将inputs转化为Dataset对象\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(inputs)\n",
    "\n",
    "    # batch the examples\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# 自定义模型,重写4个参数\n",
    "def my_model(features, labels, mode, params):\n",
    "    # 定义输入层\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns'])\n",
    "    # 定义隐藏层\n",
    "    for units in params['hidden_units']:\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "\n",
    "    # 定义输出层\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None)\n",
    "\n",
    "    # 计算predictions,返回tf.estimator.EstimatorSpec\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    \n",
    "    # 模型预测\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # 定义predictions输出哪些内容\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis],\n",
    "            'probabilities': tf.nn.softmax(logits),\n",
    "            'logits': logits,\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "\n",
    "    # 计算loss.\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # 计算evaluation metrics.\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,\n",
    "                                   predictions=predicted_classes,\n",
    "                                   name='acc_op')\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    tf.summary.scalar('accuracy', accuracy[1])\n",
    "    \n",
    "    # 评估模型\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # Create training op.\n",
    "    assert mode == tf.estimator.ModeKeys.TRAIN\n",
    "\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 获取数据\n",
    "    (train_x, train_y), (test_x, test_y) = load_data()\n",
    "\n",
    "    # 定义特征列\n",
    "    # key是train_x的列名：SepalLength，SepalWidth，PetalLength，PetalWidth\n",
    "    my_feature_columns = []\n",
    "    for key in train_x.keys():\n",
    "        my_feature_columns.append(tf.feature_column.numeric_column(key=key))\n",
    "\n",
    "    my_checkpointing_config = tf.estimator.RunConfig(\n",
    "        save_checkpoints_steps = 100,  # 每100步保存，不能和save_checkpoints_secs同时设置\n",
    "        keep_checkpoint_max = 3,       # 只保留最近3个checkpoints.\n",
    "    )\n",
    "    \n",
    "    # 建立2个隐藏层DNN的模型，每层10个隐藏节点\n",
    "    classifier = tf.estimator.Estimator(\n",
    "        model_fn=my_model,\n",
    "        params={\n",
    "            'feature_columns': my_feature_columns,\n",
    "            'hidden_units': [10, 10],\n",
    "            'n_classes': 3,\n",
    "        },\n",
    "        model_dir='models/iris',  # 保存模型\n",
    "        config=my_checkpointing_config\n",
    "    )\n",
    "\n",
    "    # 训练模型\n",
    "    classifier.train(\n",
    "        input_fn=lambda: train_input_fn(train_x, train_y, batch_size),\n",
    "        steps=train_steps)\n",
    "\n",
    "    # 评估模型\n",
    "    eval_result = classifier.evaluate(\n",
    "        input_fn=lambda: eval_input_fn(test_x, test_y, batch_size))\n",
    "\n",
    "    print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))\n",
    "\n",
    "    # 进行预测\n",
    "    # predict_x里的1列为1个样本，有3列共3个样本，expected是3个样本的真实label\n",
    "    expected = ['Setosa', 'Versicolor', 'Virginica']\n",
    "    predict_x = {\n",
    "        'SepalLength': [5.1, 5.9, 6.9],\n",
    "        'SepalWidth': [3.3, 3.0, 3.1],\n",
    "        'PetalLength': [1.7, 4.2, 5.4],\n",
    "        'PetalWidth': [0.5, 1.5, 2.1],\n",
    "    }\n",
    "\n",
    "    predictions = classifier.predict(\n",
    "        input_fn=lambda: eval_input_fn(predict_x, labels=None, batch_size=batch_size))\n",
    "\n",
    "    for pred_dict, expec in zip(predictions, expected):\n",
    "        template = ('\\nPrediction is \"{}\" ({:.1f}%), expected \"{}\"')\n",
    "\n",
    "        class_id = pred_dict['class_ids'][0]\n",
    "        probability = pred_dict['probabilities'][class_id]\n",
    "\n",
    "        print(template.format(SPECIES[class_id], 100 * probability, expec))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
